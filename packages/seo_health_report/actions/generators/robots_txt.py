"""
Robots.txt Generator

Generates an optimized robots.txt based on crawl findings.
"""

from typing import Any


def generate_robots_txt(audit_data: dict[str, Any]) -> str:
    """
    Generate an optimized robots.txt based on audit findings.

    Args:
        audit_data: Technical audit data including crawl results

    Returns:
        Complete robots.txt content as a string
    """
    lines = [
        "# Robots.txt generated by SEO Health Report System",
        "# Review and customize before deploying",
        "",
        "User-agent: *",
    ]

    # Standard disallow rules for common non-content paths
    disallow_paths = [
        "/admin/",
        "/wp-admin/",
        "/login/",
        "/cart/",
        "/checkout/",
        "/account/",
        "/search/",
        "/api/",
        "/*?sort=",
        "/*?filter=",
        "/*?session=",
    ]

    crawl_data = audit_data.get("components", {}).get("crawlability", {})

    # Add disallows from audit findings
    findings = crawl_data.get("findings", [])
    for finding in findings:
        if isinstance(finding, str) and "blocked" in finding.lower():
            # Preserve existing blocked paths
            pass

    for path in disallow_paths:
        lines.append(f"Disallow: {path}")

    lines.extend([
        "",
        "# Allow important crawlers full access",
        "User-agent: Googlebot",
        "Allow: /",
        "",
        "User-agent: Bingbot",
        "Allow: /",
        "",
        "# AI crawlers - allow for visibility",
        "User-agent: ChatGPT-User",
        "Allow: /",
        "",
        "User-agent: anthropic-ai",
        "Allow: /",
        "",
        "User-agent: PerplexityBot",
        "Allow: /",
        "",
        "User-agent: Google-Extended",
        "Allow: /",
        "",
    ])

    # Add sitemap reference
    url = audit_data.get("url", "https://example.com")
    if not url.endswith("/"):
        url += "/"
    lines.append(f"Sitemap: {url}sitemap.xml")
    lines.append("")

    return "\n".join(lines)
