"""
Sitemap XML Generator

Generates an XML sitemap from crawl data.
"""

import html
from datetime import datetime
from typing import Any


def generate_sitemap_xml(audit_data: dict[str, Any]) -> str:
    """
    Generate an XML sitemap from audit crawl data.

    Args:
        audit_data: Audit data including crawled URLs

    Returns:
        Complete sitemap.xml content as a string
    """
    urls = _extract_crawled_urls(audit_data)
    today = datetime.now().strftime("%Y-%m-%d")

    lines = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<!-- Sitemap generated by SEO Health Report System -->',
        '<!-- Review and customize before deploying -->',
        '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">',
    ]

    for url_info in urls:
        url = html.escape(url_info["url"])
        priority = url_info.get("priority", "0.5")
        changefreq = url_info.get("changefreq", "monthly")

        lines.extend([
            "  <url>",
            f"    <loc>{url}</loc>",
            f"    <lastmod>{today}</lastmod>",
            f"    <changefreq>{changefreq}</changefreq>",
            f"    <priority>{priority}</priority>",
            "  </url>",
        ])

    lines.append("</urlset>")

    return "\n".join(lines)


def _extract_crawled_urls(audit_data: dict[str, Any]) -> list[dict[str, Any]]:
    """Extract URLs from audit data for sitemap generation."""
    urls = []
    base_url = audit_data.get("url", "")

    if base_url:
        # Homepage always gets highest priority
        urls.append({
            "url": base_url,
            "priority": "1.0",
            "changefreq": "daily"
        })

    # Extract from technical audit crawl data
    technical = audit_data.get("audits", {}).get("technical", {})
    if isinstance(technical, dict):
        components = technical.get("components", {})

        # Get crawled pages
        crawlability = components.get("crawlability", {})
        if isinstance(crawlability, dict):
            crawled_pages = crawlability.get("pages_crawled", [])
            for page in crawled_pages:
                page_url = page if isinstance(page, str) else page.get("url", "")
                if page_url and page_url != base_url:
                    urls.append({
                        "url": page_url,
                        "priority": "0.8",
                        "changefreq": "weekly"
                    })

    # Extract from content audit pages
    content = audit_data.get("audits", {}).get("content", {})
    if isinstance(content, dict):
        components = content.get("components", {})
        content_quality = components.get("content_quality", {})
        if isinstance(content_quality, dict):
            pages = content_quality.get("pages_analyzed", [])
            for page in pages:
                page_url = page if isinstance(page, str) else page.get("url", "")
                if page_url and page_url not in [u["url"] for u in urls]:
                    urls.append({
                        "url": page_url,
                        "priority": "0.6",
                        "changefreq": "monthly"
                    })

    return urls
